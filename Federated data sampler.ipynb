{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Part 1 mnist sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mnist_iid(dataset, num_users):    \n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
    "                                             replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "def mnist_noniid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 60,000 training imgs -->  200 imgs/shard X 300 shards\n",
    "    num_shards, num_imgs = 200, 300\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    labels = dataset.targets.numpy()\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # divide and assign 2 shards/client\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate(\n",
    "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users\n",
    "\n",
    "def mnist_noniid_unequal(dataset, num_users, cifar=False):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset s.t clients\n",
    "    have unequal amount of data\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :returns a dict of clients with each clients assigned certain\n",
    "    number of training imgs\n",
    "    \"\"\"\n",
    "\n",
    "    # 60,000 training imgs --> 50 imgs/shard X 1200 shards\n",
    "    num_shards, num_imgs = 300, 200# origin 1200, 50\n",
    "    # add condition to be reused by cifar\n",
    "    if cifar:\n",
    "        num_shards, num_imgs = 250, 200# origin 1000, 50\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    labels = np.array(dataset.targets)\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # Minimum and maximum shards assigned per client:\n",
    "    min_shard = 1 \n",
    "    max_shard = 3 # original is 30\n",
    "\n",
    "    # Divide the shards into random chunks for every client\n",
    "    # s.t the sum of these chunks = num_shards\n",
    "    random_shard_size = np.random.randint(min_shard, max_shard+1,\n",
    "                                          size=num_users)\n",
    "    random_shard_size = np.around(random_shard_size /\n",
    "                                  sum(random_shard_size) * num_shards)\n",
    "    random_shard_size = random_shard_size.astype(int)\n",
    "\n",
    "    # Assign the shards randomly to each client\n",
    "    if sum(random_shard_size) > num_shards:\n",
    "\n",
    "        for i in range(num_users):\n",
    "            # First assign each client 1 shard to ensure every client has\n",
    "            # atleast one shard of data\n",
    "            rand_set = set(np.random.choice(idx_shard, 1, replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "        random_shard_size = random_shard_size-1\n",
    "\n",
    "        # Next, randomly assign the remaining shards\n",
    "        for i in range(num_users):\n",
    "            if len(idx_shard) == 0:\n",
    "                continue\n",
    "            shard_size = random_shard_size[i]\n",
    "            if shard_size > len(idx_shard):\n",
    "                shard_size = len(idx_shard)\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "    else:\n",
    "\n",
    "        for i in range(num_users):\n",
    "            shard_size = random_shard_size[i]\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "        if len(idx_shard) > 0:\n",
    "            # Add the leftover shards to the client with minimum images:\n",
    "            shard_size = len(idx_shard)\n",
    "            # Add the remaining shard to the client with lowest data\n",
    "            k = min(dict_users, key=lambda x: len(dict_users.get(x)))\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[k] = np.concatenate(\n",
    "                    (dict_users[k], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### get_mnist_dataset() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/clairegong/Federated-Learning-PyTorch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%cd /Users/clairegong/Federated-Learning-PyTorch/\n",
    "\n",
    "def get_mnist_dataset(args):\n",
    "    \"\"\" Returns train and test datasets and a user group which is a dict where\n",
    "    the keys are the user index and the values are the corresponding data for\n",
    "    each of those users.\n",
    "    \"\"\"\n",
    "\n",
    "    apply_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    data_dir = './data/mnist/'\n",
    "\n",
    "    train_dataset = datasets.MNIST(data_dir, train=True, download=True,\n",
    "                                   transform=apply_transform)\n",
    "\n",
    "    test_dataset = datasets.MNIST(data_dir, train=False, download=True,\n",
    "                                  transform=apply_transform)\n",
    "\n",
    "    # sample training data amongst users\n",
    "    if args['iid']:\n",
    "        # Sample IID user data from Mnist\n",
    "        user_groups = mnist_iid(train_dataset, args['num_users'])\n",
    "    else:\n",
    "        # Sample Non-IID user data from Mnist\n",
    "        if args['unequal']:\n",
    "            # Chose uneuqal splits for every user\n",
    "            user_groups = mnist_noniid_unequal(train_dataset, args['num_users'])\n",
    "        else:\n",
    "            # Chose euqal splits for every user\n",
    "            user_groups = mnist_noniid(train_dataset, args['num_users'])\n",
    "\n",
    "    return train_dataset, test_dataset, user_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### test MNIST index sampling\n",
    "- user_groups is a set\n",
    "\n",
    "#### MNIST iid: COCO iid can directly copy\n",
    "\n",
    "#### MNIST niid equal sampler:\n",
    "1. divide data into num_shards\n",
    "2. sort data idx according img labels/classes\n",
    "3. return each user 2 shards of imag idxs; thus 1 dict of num_user idx lists\n",
    "\n",
    "#### MNIST niid unequal sampler(more realistic to real scenario and performance good):\n",
    "1. same\n",
    "2. same\n",
    "3. !set shard_per_client range\n",
    "4. assign dict of num_user idx lists to users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "args = {}\n",
    "args['iid'] = 1\n",
    "args['unequal'] = 1\n",
    "args['num_users'] = 100\n",
    "\n",
    "_, _, user_groups = get_mnist_dataset(args)\n",
    "print(len(user_groups))\n",
    "idx = randint(0, len(user_groups)-1)\n",
    "print(len(user_groups[idx]))\n",
    "# print(user_groups[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "args['iid'] = 0\n",
    "args['unequal'] = 0\n",
    "_, _, user_groups = get_mnist_dataset(args)\n",
    "print(len(user_groups))\n",
    "idx = randint(0, len(user_groups)-1)\n",
    "print(len(user_groups[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "args['iid'] = 0\n",
    "args['unequal'] = 1\n",
    "_, _, user_groups = get_mnist_dataset(args)\n",
    "print(len(user_groups))\n",
    "idx = randint(0, len(user_groups)-1)\n",
    "print(len(user_groups[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Part 2 Make coco datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/clairegong/Federated-Learning-PyTorch/cocotools\n",
      "loading annotations into memory...\n",
      "Done (t=0.54s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.57s)\n",
      "creating index...\n",
      "index created!\n",
      "4000\n",
      "1000\n",
      "1000\n",
      "250\n",
      "torch.Size([4, 3, 480, 480])\n",
      "torch.Size([4, 480, 480])\n"
     ]
    }
   ],
   "source": [
    "%cd '/Users/clairegong/Federated-Learning-PyTorch/cocotools'\n",
    "path2data = '../data/coco/val2017'\n",
    "path2ann = '../data/coco/annotations/instances_val2017.json'\n",
    "\n",
    "from train import train_one_epoch, evaluate, criterion, get_transform\n",
    "from coco_utils import ConvertCocoPolysToMask, FilterAndRemapCocoCategories\n",
    "from transforms import Compose\n",
    "import utils\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "\n",
    "catIds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, \\\n",
    "          21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\\\n",
    "          42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, \\\n",
    "          61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n",
    "\n",
    "train_data = datasets.CocoDetection(path2data, path2ann, transforms=Compose([FilterAndRemapCocoCategories(catIds, remap=True), \n",
    "                                                                              ConvertCocoPolysToMask(),\n",
    "                                                                              get_transform(train=True)]))\n",
    "test_data = datasets.CocoDetection(path2data, path2ann, transforms=Compose([FilterAndRemapCocoCategories(catIds, remap=True), \n",
    "                                                                    ConvertCocoPolysToMask(),\n",
    "                                                                    get_transform(train=False)]))\n",
    "\n",
    "# # split train and test indice\n",
    "torch.manual_seed(1)\n",
    "idxs = torch.randperm(len(train_data)).tolist()\n",
    "train_data = torch.utils.data.Subset(train_data, idxs[:4000])\n",
    "test_data = torch.utils.data.Subset(test_data, idxs[4000:])\n",
    "\n",
    "# following references.sementation.train.main setting\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=True, num_workers=4,\\\n",
    "                                        collate_fn=utils.collate_fn, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=4, shuffle=False, num_workers=4,\\\n",
    "                                          collate_fn=utils.collate_fn)\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(len(train_loader))\n",
    "print(len(test_loader)) \n",
    "\n",
    "# check IF loader works\n",
    "img, target = iter(train_loader).next()\n",
    "print(img.size())\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  16  60  64  74  78 255]\n",
      "[121820, 4022, 74017, 2182, 17452, 928, 9979]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx= randint(0, len(train_data)-1)\n",
    "target = train_data[idx][1]\n",
    "target_class = np.unique(target.numpy())\n",
    "pixels_per_class = []\n",
    "for c in target_class:\n",
    "    pixels = np.where(target == c)[0].size\n",
    "    pixels_per_class.append(pixels)\n",
    "print(target_class)\n",
    "print(pixels_per_class)\n",
    "sum(pixels_per_class) == 480**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make coco sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functon, spend 40mins on this, too long!!!\n",
    "\n",
    "def convert_coco_mask_to_top_class(dataset):\n",
    "    # return the numpy array of top class of each img\n",
    "    targets = [] \n",
    "    for (_, target) in dataset:\n",
    "        classes = np.unique(target.numpy()) # a sorted array\n",
    "         # remove background class 0, 255\n",
    "        if len(classes) and classes[0] == 0:\n",
    "            classes = classes[1:]\n",
    "        if len(classes) and classes[-1] == 255:\n",
    "            classes = classes[:-1]\n",
    "\n",
    "        if len(classes) == 0:\n",
    "            targets.append(0)\n",
    "        elif len(classes) == 1:\n",
    "            targets.append(classes[0])\n",
    "        else:\n",
    "            pixels_per_class = []\n",
    "            for c in classes:\n",
    "                pixels = len(np.where(target==c)[0])\n",
    "                pixels_per_class.append(pixels)\n",
    "            # get the top class with most pixels\n",
    "            top_class = classes[np.argmax(pixels_per_class)]\n",
    "            targets.append(top_class) \n",
    "    return np.array(targets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_iid(dataset, num_users):    \n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
    "                                             replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n",
    "\n",
    "def coco_noniid(dataset, num_users, labels):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 60,000 training imgs -->  200 imgs/shard X 300 shards\n",
    "    num_shards = 200\n",
    "    num_imgs = len(dataset) // num_shards \n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # divide and assign 2 shards/client\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate(\n",
    "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n",
    "    return dict_users\n",
    "\n",
    "def coco_noniid_unequal(dataset, num_users, labels):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from MNIST dataset s.t clients\n",
    "    have unequal amount of data\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :returns a dict of clients with each clients assigned certain\n",
    "    number of training imgs\n",
    "    \"\"\"\n",
    "\n",
    "    # 60,000 training imgs --> 50 imgs/shard X 1200 shards\n",
    "    num_shards = 1000\n",
    "    num_imgs = len(dataset) // num_shards\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # Minimum and maximum shards assigned per client:\n",
    "    min_shard = 1 \n",
    "    max_shard = 30 # original is 30\n",
    "\n",
    "    # Divide the shards into random chunks for every client\n",
    "    # s.t the sum of these chunks = num_shards\n",
    "    random_shard_size = np.random.randint(min_shard, max_shard+1,\n",
    "                                          size=num_users)\n",
    "    random_shard_size = np.around(random_shard_size /\n",
    "                                  sum(random_shard_size) * num_shards)\n",
    "    random_shard_size = random_shard_size.astype(int)\n",
    "\n",
    "    # Assign the shards randomly to each client\n",
    "    if sum(random_shard_size) > num_shards:\n",
    "\n",
    "        for i in range(num_users):\n",
    "            # First assign each client 1 shard to ensure every client has\n",
    "            # atleast one shard of data\n",
    "            rand_set = set(np.random.choice(idx_shard, 1, replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "        random_shard_size = random_shard_size-1\n",
    "\n",
    "        # Next, randomly assign the remaining shards\n",
    "        for i in range(num_users):\n",
    "            if len(idx_shard) == 0:\n",
    "                continue\n",
    "            shard_size = random_shard_size[i]\n",
    "            if shard_size > len(idx_shard):\n",
    "                shard_size = len(idx_shard)\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "    else:\n",
    "\n",
    "        for i in range(num_users):\n",
    "            shard_size = random_shard_size[i]\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "        if len(idx_shard) > 0:\n",
    "            # Add the leftover shards to the client with minimum images:\n",
    "            shard_size = len(idx_shard)\n",
    "            # Add the remaining shard to the client with lowest data\n",
    "            k = min(dict_users, key=lambda x: len(dict_users.get(x)))\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[k] = np.concatenate(\n",
    "                    (dict_users[k], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test COCO index sampling\n",
    "- user_groups is a set\n",
    "\n",
    "#### MNIST iid: COCO iid can directly copy\n",
    "\n",
    "#### MNIST niid equal sampler:\n",
    "1. divide data into num_shards\n",
    "2. sort data idx according img labels/classes\n",
    "3. return each user 2 shards of imag idxs; thus 1 dict of num_user idx lists\n",
    "\n",
    "#### MNIST niid unequal sampler(more realistic to real scenario and performance good):\n",
    "1. same\n",
    "2. same\n",
    "3. !set shard_per_client range\n",
    "4. assign dict of num_user idx lists to users\n",
    "\n",
    "#### COCO niid unequal sampler (sort by top class)\n",
    "1. same\n",
    "2. make helper function to sort idx based on top class in a image\n",
    "3. same\n",
    "4. same\n",
    "\n",
    "#### COCO niid unequal sampler (one shard one top class)\n",
    "1. skip\n",
    "2. make helper function to sort idx based on top class in a image\n",
    "3. make each class a shard\n",
    "4. same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco_user_groups(args, train_dataset, labels):\n",
    "    \"\"\" Returns train and test datasets and a user group which is a dict where\n",
    "    the keys are the user index and the values are the corresponding data for\n",
    "    each of those users.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # sample training data amongst users\n",
    "    if args['iid']:\n",
    "        # Sample IID user data from coco\n",
    "        user_groups = coco_iid(train_dataset, args['num_users'])\n",
    "    else:\n",
    "        # Sample Non-IID user data from coco\n",
    "        if args['unequal']:\n",
    "            # Chose uneuqal splits for every user\n",
    "            user_groups = coco_noniid_unequal(train_dataset, args['num_users'], labels)\n",
    "        else:\n",
    "            # Chose euqal splits for every user\n",
    "            user_groups = coco_noniid(train_dataset, args['num_users'], labels)\n",
    "\n",
    "    return user_groups\n",
    "\n",
    "labels = convert_coco_mask_to_top_class(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "40\n",
      "[1547. 3176. 2946. 2218. 3424. 1367. 2071. 3938. 3861. 1092. 3339. 3559.\n",
      " 2552. 3342. 3876. 3083. 1742. 1118. 1721.  489.  404. 2178. 2174. 3790.\n",
      "  290. 3883.  304. 2759. 3272.  593. 2832.  315.  620.  338.  626. 2839.\n",
      " 3165.  353. 3746. 1902.]\n"
     ]
    }
   ],
   "source": [
    "# labels takes too long to compute, should save for reuse\n",
    "args['iid'] = 0\n",
    "args['unequal'] = 0\n",
    "user_groups = get_coco_user_groups(args, train_data, labels)\n",
    "print(len(user_groups))\n",
    "idx = randint(0, len(user_groups)-1)\n",
    "print(len(user_groups[idx]))\n",
    "print(user_groups[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "20\n",
      "[ 260.  940.  564. 1684. 1382.  958. 2526. 3635.   35. 3768. 2774. 2868.\n",
      " 2102. 2104. 2219. 2216. 1233. 1232. 1231. 2573.]\n"
     ]
    }
   ],
   "source": [
    "args['iid'] = 0\n",
    "args['unequal'] = 1\n",
    "user_groups = get_coco_user_groups(args, train_data, labels)\n",
    "print(len(user_groups))\n",
    "idx = randint(0, len(user_groups)-1)\n",
    "print(len(user_groups[idx]))\n",
    "print(user_groups[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "40\n",
      "{1152, 389, 3975, 1804, 3983, 784, 657, 402, 2454, 2712, 537, 155, 3742, 1825, 2342, 295, 1194, 3120, 2682, 2488, 2489, 3904, 3139, 1091, 3659, 1484, 3663, 2898, 87, 2521, 3545, 3547, 1505, 1890, 106, 1651, 3445, 1655, 3834, 1147}\n"
     ]
    }
   ],
   "source": [
    "args['iid'] = 1\n",
    "args['unequal'] = 0\n",
    "user_groups = get_coco_user_groups(args, train_data, labels)\n",
    "print(len(user_groups))\n",
    "idx = randint(0, len(user_groups)-1)\n",
    "print(len(user_groups[idx]))\n",
    "print(user_groups[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "fl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
